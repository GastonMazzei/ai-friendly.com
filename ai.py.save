# AI
import numpy as np
import pandas as pd
import sys

from sklearn import preprocessing, metrics
from sklearn.model_selection import train_test_split
from keras.models import Sequential, load_model
from keras.layers import Dense

from keras import backend

# extras
import extras
# plots
import matplotlib.pyplot as plt

def process(dir):
    print(' ========== AI PROCESS ========== ')
    LCFILE = extras.LearnCardFile(dir)
    
    df = pd.read_excel(dir + r'/%s'%LCFILE, header=0)

    dataset = df.values
    dimensions = dataset.shape[1] -1

    X = dataset[:,0:dimensions]
    Y = dataset[:,dimensions]

    min_max_scaler = preprocessing.MinMaxScaler()
    X_scale = min_max_scaler.fit_transform(X)

    X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)
    X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)

    model = Sequential([
        Dense(32, activation='relu', input_shape=(dimensions,)),
        Dense(32, activation='relu'),
        Dense(1, activation='sigmoid'),
    ])

    model.compile(optimizer='sgd',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    hist = model.fit(X_train, Y_train,
              batch_size=32, epochs=100, verbose=0,
              validation_data=(X_val, Y_val))


    confidence = model.evaluate(X_test, Y_test, verbose=0)[1]
    result = {}
    result['confidence'] = confidence
    
    #save model
    model.save(dir + r'/model.h5')
    
    S = extras.uploadedFilesFilenameSize
    ICFILE = extras.InCardsFiles(dir)[0]
    
    testdata = pd.read_excel(dir + r'/%s'%ICFILE, header=0)
    testdata = testdata.values
    testdata_scale = min_max_scaler.fit_transform(testdata)
    result = model.predict_classes(testdata_scale)
    output = np.hstack((testdata, result))
	
    nIC = extras.nCardFile(ICFILE)
    OCFILE = "OC_%03i_%s"%(nIC, ICFILE[S:])
    pd.DataFrame(output).to_excel(dir + '/%s'%OCFILE)

# PLOTS <<<<<<
    plotCaoba(X_test, Y_test, model)
    plt.title('AI Friendly performance')
    plt.savefig(dir + r'/fig1.png', bbox_inches='tight')
    #plt.show()
    plt.close()
    
    backend.clear_session()
    return OCFILE

def loadModelAndProcess(_dir, ICFILE):
    model = load_model(_dir + r'/model.h5')
    
    S = extras.uploadedFilesFilenameSize
	
    LCFILE = extras.LearnCardFile(_dir)
    df = pd.read_excel(_dir + r'/%s'%LCFILE, header=0)
    dataset = df.values
    dimensions = dataset.shape[1] -1
    X = dataset[:,0:dimensions]
    min_max_scaler = preprocessing.MinMaxScaler()
    X_scale = min_max_scaler.fit_transform(X)
    
    testdata = pd.read_excel(_dir + r'/%s'%ICFILE, header=0)
    testdata = testdata.values
    testdata_scale = min_max_scaler.fit_transform(testdata)
    result = model.predict_classes(testdata_scale)
    output = np.hstack((testdata, result))
	
    nIC = extras.nCardFile(ICFILE)
    pd.DataFrame(output).to_excel(_dir + "/OC_%03i_%s"%(nIC, ICFILE[S:]))
	
    backend.clear_session()
    return
	
def plotCaoba(X_test, Y_test, model):
    y_true = Y_test
    Ytoshuffle=np.zeros(len(Y_test))
    for i in range(len(Y_test)):
        Ytoshuffle[i]=Y_test[i]
    np.random.shuffle(Ytoshuffle)
    y_scoresrandom = Ytoshuffle
    print('AUC Random tagger %8.3f \n' % metrics.roc_auc_score(y_true, y_scoresrandom))
    fprr, tprr, thresholdsr = metrics.roc_curve(y_true, y_scoresrandom)
    plt.plot(fprr,tprr, label = 'random')
    y_scores = model.predict_proba(X_test)
    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_scores)
    plt.plot(fpr,tpr, label = 'NN AUC '+ str(round(metrics.roc_auc_score(y_true, y_scores),2)))
    plt.xlabel('False Positive rate')
    plt.ylabel('True Positive rate')
    plt.yscale('linear')
    plt.legend(loc = 'lower right')
    return
	
if __name__ == '__main__':
    import sys
    process(str(sys.argv[1]))
